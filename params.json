{"name":"Practical Machine Learning Project","tagline":"Project for Coursera class from Data Science specialization.","body":"### Practical Machine learning project.\r\n\r\n## Background\r\n\r\nThe data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, doing the set of exercises, was collected, processed and published by [this group](http://groupware.les.inf.puc-rio.br/har).\r\nParticipant were asked to perform barbell lifts correctly and incorrectly in 5 different ways (denoted by letters A, B, C, D or E). Training data set can be downloaded from [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), and test data set - from [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) \r\n \r\n## Data cleaning\r\n\r\nI have downloaded both csv files into my working directory and uploaded them to R workspace as data frames. All empty fields were filled with NA. Outcome A, B, C, D, E is stored in \"classe\" column.\r\n\r\n```\r\nlibrary(caret)\r\n\r\n## load datasets\r\n\r\nfull <- read.csv(\"pml-training.csv\", stringsAsFactors = TRUE,\r\n                                     na.strings = c(\"\", \"NA\"))\r\ntesting <- read.csv(\"pml-testing.csv\", stringsAsFactors = TRUE,\r\n                                       na.strings = c(\"\", \"NA\"))\r\n```\r\n\r\nTest set does not have answers, so I need cross validation to estimate model accuracy. The data set is large enough to split it into training (70%) and validation (30%) data sets, which are both still fairly large.\r\nI also set seed for reproducibility.\r\n\r\n```\r\n# split full data set into training and validation\r\n\r\nset.seed(123)\r\ninTrain <- createDataPartition(full$classe, p = 0.7, list = FALSE)\r\ntraining <- full[inTrain, ]\r\nvalidation <- full[-inTrain, ]\r\n```\r\n\r\nNot all variables in the data set would make good predictors. So first I make a list of variable names that I want to drop from the data.\r\nVariables \"X\", \"user_name\", and \"problem_id\" should not affect the prediction. Since I build classification rather than forecasting model, I should also drop all variables related to time (\"raw_timestamp_part_1\",  \"raw_timestamp_part_2\", \"cvtd_timestamp\", \"new_window\" and \"num_window\").\r\n\r\n```\r\n# list of columns that should not affect the prediction\r\n\r\nskipColumns <- c(\"X\", \r\n                 \"user_name\", \r\n                 \"raw_timestamp_part_1\", \r\n                 \"raw_timestamp_part_2\", \r\n                 \"cvtd_timestamp\", \r\n                 \"new_window\", \r\n                 \"num_window\",\r\n                 \"problem_id\")\r\n```\r\n\r\nSome columns miss too many values. I am going to drop all columns that have more than 50% of values missing.\r\n\r\n```\r\n# add columns that miss more than half of the values in training dataset\r\n\r\nfor (i in 0:dim(training)[2])\r\n    {\r\n    if (sum(is.na(training[ , i])) > 0.5 * length(training[ , i]))\r\n        {\r\n        skipColumns <- c(skipColumns, names(training[i]))\r\n        }\r\n    }\r\n```\r\n\r\nNow that I have the full list of irrelevant variables, I can delete them from my data frames.\r\n\r\n```\r\n# reduce number of predictors\r\n\r\ntraining <- training[, ! names(training) %in% skipColumns]\r\nvalidation <- validation[, ! names(validation) %in% skipColumns]\r\ntesting <- testing[, ! names(testing) %in% skipColumns]\r\n```\r\n\r\n## Model building\r\n\r\nI build random forest model on clean training data set. It shall predict \"classe\" based on all other variables in data frame.\r\n\r\n```\r\n# build a random forest model\r\n\r\nmodFit <- train(classe ~ ., data = training, model = \"rf\")\r\n```\r\n\r\nI first want to check its accuracy on training data:\r\n\r\n```\r\n# run model on training data set, get accuracy on training set \r\n\r\ntrainPrediction <- predict(modFit, training)\r\nprint(confusionMatrix(training$classe, trainPrediction))\r\n```\r\n\r\nThis is what I get (truncated for brevity):\r\n\r\n```\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 3906    0    0    0    0\r\n         B    0 2658    0    0    0\r\n         C    0    0 2396    0    0\r\n         D    0    0    0 2252    0\r\n         E    0    0    0    0 2525\r\n\r\nOverall Statistics\r\n                                     \r\n               Accuracy : 1          \r\n                 95% CI : (0.9997, 1)\r\n    No Information Rate : 0.2843     \r\n    P-Value [Acc > NIR] : < 2.2e-16  \r\n                                     \r\n                  Kappa : 1          \r\n Mcnemar's Test P-Value : NA         \r\n```\r\nNot bad! The model predicts all the outcomes in training data with 100% accuracy. But this is not a correct estimate of the model accuracy, because the model might be overfitted on training dataset. \r\nI now apply my model to the validation data:\r\n\r\n```\r\n# run model on validation data set, estimate out of the sample error\r\n\r\nvalidPrediction <- predict(modFit, validation)\r\nprint(confusionMatrix(validation$classe, validPrediction))\r\n```\r\nAs expected, the confusion matrix is no longer strictly diagonal, and accuracy is below 100%:\r\n\r\n```\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 1672    2    0    0    0\r\n         B    7 1130    2    0    0\r\n         C    0    8 1015    3    0\r\n         D    0    0   17  946    1\r\n         E    0    0    2    1 1079\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9927          \r\n                 95% CI : (0.9902, 0.9947)\r\n    No Information Rate : 0.2853          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9908          \r\n Mcnemar's Test P-Value : NA          \r\n```\r\n\r\nStill, out of the sample error is less than 1%, so the model is pretty good!\r\n\r\n## Submission\r\n\r\nAll that's left is to run the model on training set and write the answer for each of 20 cases into a separate file:\r\n\r\n```\r\n# run model on testing data set\r\n\r\nanswers <- predict(modFit, testing)\r\nprint(answers)\r\n\r\n# write answers to individual files\r\n\r\nfor(i in 1:length(answers)) {\r\n    filename = paste0(\"problem_id_\", i, \".txt\")\r\n    write.table(answers[i], file=filename,\r\n                quote=FALSE, row.names=FALSE, col.names=FALSE)\r\n}\r\n```\r\nOut of sample accuracy 99% and a little bit of luck is enough to get 20 out of 20:\r\n\r\n```\r\n [1] B A B A A E D B A A B C B A E E A B B B\r\nLevels: A B C D E\r\n```\r\n\r\nThe End.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}